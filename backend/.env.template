# Api keys and model urls
# [OPTIONAL] OpenAI API key for GPT models
OPENAI_API_KEY=
# [OPTIONAL] Anthropic API key for Claude models
ANTHROPIC_API_KEY=
# [OPTIONAL] Azure OpenAI API key
AZURE_API_KEY=
# [CONDITIONAL] Required if using Azure models
AZURE_ENDPOINT=
# [CONDITIONAL] Same as AZURE_ENDPOINT, required for LiteLLM Azure models
AZURE_API_BASE=
# [CONDITIONAL] Required if using Azure models (e.g., gpt-4)
AZURE_MODEL_DEPLOYMENT=
# [CONDITIONAL] Required if using Azure models (e.g., 2023-05-15)
AZURE_API_VERSION=
# [OPTIONAL] OVHCloud API key
OVHCLOUD_API_KEY=
# [OPTIONAL] Mistral AI API key
MISTRAL_API_KEY=
# [OPTIONAL] Berget.ai API key for Swedish-hosted models
BERGET_API_KEY=
# [OPTIONAL] Berget.ai API base URL (default: https://api.berget.ai/v1)
BERGET_API_BASE=
# [OPTIONAL] Flux API key for image generation
FLUX_API_KEY=
# [OPTIONAL] Tavily API key for web search
TAVILY_API_KEY=
# [OPTIONAL] Self-hosted embedding service URL
INFINITY_URL=
# [OPTIONAL] vLLM model service URL
VLLM_MODEL_URL=
# [OPTIONAL] vLLM API key (for self hosted models)
VLLM_API_KEY=

# Marketplace and internal services
# [OPTIONAL] Internal marketplace API key
INTRIC_MARKETPLACE_API_KEY=
# [OPTIONAL] Internal marketplace URL
INTRIC_MARKETPLACE_URL=
# [OPTIONAL] Internal super API key
INTRIC_SUPER_API_KEY=
# [OPTIONAL] Internal super duper API key
INTRIC_SUPER_DUPER_API_KEY=

# Infrastructure dependencies
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_PORT=5432
POSTGRES_HOST=localhost
POSTGRES_DB=postgres
REDIS_HOST=localhost
REDIS_PORT=6379

# Authentication dependencies
# [CONDITIONAL] Required if using MobilityGuard authentication
MOBILITYGUARD_DISCOVERY_ENDPOINT=
# [CONDITIONAL] Required if using MobilityGuard authentication
MOBILITYGUARD_CLIENT_ID=
# [CONDITIONAL] Required if using MobilityGuard authentication
MOBILITYGUARD_CLIENT_SECRET=
# [OPTIONAL] MobilityGuard tenant ID
MOBILITYGUARD_TENANT_ID=

# Max sizes
UPLOAD_FILE_TO_SESSION_MAX_SIZE=10485760
UPLOAD_IMAGE_TO_SESSION_MAX_SIZE=10485760
UPLOAD_MAX_FILE_SIZE=10485760
TRANSCRIPTION_MAX_FILE_SIZE=10485760
MAX_IN_QUESTION=1

# Feature flags
# [OPTIONAL] Enable access management features (default: True)
USING_ACCESS_MANAGEMENT=True
# [OPTIONAL] Enable Azure OpenAI models (default: False)
USING_AZURE_MODELS=False
# [OPTIONAL] Enable Identity and Access Management (default: False)
USING_IAM=False
# [OPTIONAL] Enable image generation features (default: False)
USING_IMAGE_GENERATION=False
# [OPTIONAL] Enable web crawling features (default: True)
USING_CRAWL=True
# [OPTIONAL] Enable testing mode (default: False)
TESTING=False
# [OPTIONAL] Enable development mode (default: False)
DEV=False

###############################################################################
# WEB CRAWLING SETTINGS
###############################################################################

## General Crawler Settings (applies to ALL engines: Scrapy and Crawl4AI)
# [OPTIONAL] Maximum crawl duration in seconds (default: 14400 = 4 hours)
CRAWL_MAX_LENGTH=14400
# [OPTIONAL] Maximum items to crawl before stopping (default: 20000)
CLOSESPIDER_ITEMCOUNT=20000
# [OPTIONAL] Respect robots.txt files (default: True)
OBEY_ROBOTS=True
# [OPTIONAL] Enable automatic request throttling (default: True)
AUTOTHROTTLE_ENABLED=True

## Crawl4AI Engine Settings (only applies when using crawler_engine=crawl4ai)

# Performance Optimization
# [OPTIONAL] Enable text-only mode for faster, lighter crawls - disables images/GPU (default: True)
# Why: Reduces memory from ~500MB to ~50MB per page, 3-4x faster crawls
# When to disable: If content requires JavaScript rendering or image context
CRAWL4AI_TEXT_ONLY_MODE=True
# [OPTIONAL] Enable light mode to reduce browser overhead (default: True)
# Why: Additional 10-20% memory reduction by disabling background features
CRAWL4AI_LIGHT_MODE=True

# Memory & Concurrency Management
# [OPTIONAL] Memory threshold percentage - auto-pause crawls when system RAM exceeds this % (default: 75.0)
# Why: Prevents server crashes when multiple Crawl4AI crawls run simultaneously
CRAWL4AI_MEMORY_THRESHOLD_PERCENT=75.0
# [OPTIONAL] Max concurrent pages in sitemap crawls - controls parallelism within a single sitemap (default: 5)
# Why: Large sitemaps (1000+ pages) process 5 pages at once instead of sequentially (5× faster)
# Memory impact: 5 concurrent pages ≈ 600MB per sitemap crawl
CRAWL4AI_MAX_CONCURRENT_SESSIONS=5
# [OPTIONAL] Memory check interval in seconds - how often to monitor system memory (default: 1.0)
CRAWL4AI_MEMORY_CHECK_INTERVAL=1.0

# Rate Limiting (prevents server blocking and 429/503 errors)
# [OPTIONAL] Minimum delay between requests to same domain in seconds (default: 1.0)
# Why: Prevents overwhelming target servers and reduces rate-limit errors
CRAWL4AI_RATE_LIMIT_DELAY_MIN=1.0
# [OPTIONAL] Maximum delay between requests to same domain in seconds (default: 3.0)
# Why: Random delays between min/max make requests less predictable and more server-friendly
CRAWL4AI_RATE_LIMIT_DELAY_MAX=3.0
# [OPTIONAL] Maximum retry attempts on rate-limit errors (429, 503) (default: 3)
# Why: Automatic exponential backoff on server rate-limit responses
CRAWL4AI_MAX_RETRIES=3

# Monitoring
# [OPTIONAL] Enable live crawl progress display in worker terminal (default: True)
# Why: Shows real-time progress table with active tasks, memory usage, and completion status
# Note: EXPERIMENTAL - Display works best in interactive terminals, may not show in Docker logs
# The monitor will display during sitemap crawls (not single-page crawls)
CRAWL4AI_ENABLE_MONITOR=True
# [OPTIONAL] Show detailed [FETCH] [SCRAPE] [COMPLETE] messages from crawl4ai (default: False)
# Why: Suppresses verbose crawl messages that can clutter logs
# Set to True if you want to see detailed step-by-step crawl progress
CRAWL4AI_SHOW_PROGRESS_MESSAGES=False

# Content Extraction
# [OPTIONAL] Minimum word count for valid page content (default: 10)
CRAWL4AI_WORD_THRESHOLD=10
# [OPTIONAL] Table extraction score threshold - lower captures more tables (default: 5)
CRAWL4AI_TABLE_SCORE_THRESHOLD=5

# File Downloads
# [OPTIONAL] File download timeout in seconds - how long to wait for downloads (default: 15)
CRAWL4AI_DOWNLOAD_TIMEOUT=15
# [OPTIONAL] Maximum file size in MB for downloaded files (default: 50)
CRAWL4AI_MAX_FILE_SIZE_MB=50

## Worker Configuration (applies to ALL crawler engines - Scrapy and Crawl4AI)
# [OPTIONAL] Maximum concurrent jobs the worker can process at once (default: 20)
# Why: Controls overall system load - reduce if server has limited RAM
# Memory impact: 20 mixed jobs ≈ 3-6GB depending on Scrapy/Crawl4AI ratio
# Recommendation: Use 10-15 for servers with <8GB RAM, 20+ for larger servers
WORKER_MAX_CONCURRENT_JOBS=20

# Integrations and callbacks
# [OPTIONAL] OAuth callback URL for integrations
OAUTH_CALLBACK_URL=
# [OPTIONAL] Confluence client ID for integration
CONFLUENCE_CLIENT_ID=
# [OPTIONAL] Confluence client secret for integration
CONFLUENCE_CLIENT_SECRET=
# [OPTIONAL] SharePoint client ID for integration
SHAREPOINT_CLIENT_ID=
# [OPTIONAL] SharePoint client secret for integration
SHAREPOINT_CLIENT_SECRET=


# Security
API_PREFIX=/api/v1
API_KEY_LENGTH=64
API_KEY_HEADER_NAME=example
JWT_AUDIENCE=*
JWT_ISSUER=EXAMPLE
JWT_EXPIRY_TIME=86000
JWT_ALGORITHM=HS256
JWT_SECRET=1234
JWT_TOKEN_PREFIX=
URL_SIGNING_KEY=

# Default tenant and user
DEFAULT_TENANT_NAME=ExampleTenant
DEFAULT_TENANT_QUOTA_LIMIT=10737418240
DEFAULT_USER_NAME=ExampleUser
DEFAULT_USER_EMAIL=user@example.com
DEFAULT_USER_PASSWORD=Password1!

# Log
# Set to INFO for clean logs with optional crawl monitor display
# Set to DEBUG for detailed debugging (note: debug logs may interfere with monitor display)
LOGLEVEL=DEBUG
