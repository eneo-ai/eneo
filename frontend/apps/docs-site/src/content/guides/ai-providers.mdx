# AI Provider Configuration

Eneo is model-agnostic and supports multiple AI providers through **LiteLLM**, a unified abstraction layer. Models are configured via the `ai_models.yml` file.

## Architecture Overview

Eneo uses a two-tier routing system for AI models:

1. **Primary (LiteLLM)**: Models with `litellm_model_name` are routed through the LiteLLM abstraction layer
2. **Fallback (Family-based)**: Models without `litellm_model_name` use legacy family-specific adapters

All model configuration is managed through a single YAML file that syncs with the database.

> **Important: Deprecated Adapters**
>
> The family-based adapters (`OpenAIModelAdapter`, `ClaudeModelAdapter`, `AzureOpenAIModelAdapter`, etc.) are **deprecated** and will not be maintained. All new model integrations should use the `litellm_model_name` field to route through LiteLLM instead of writing custom adapters.
>
> LiteLLM provides:
> - Unified API across 100+ providers
> - Automatic retries and fallbacks
> - Consistent error handling
> - Active community maintenance
>
> **Do not write new adapters** - configure models via `ai_models.yml` with `litellm_model_name` instead.

---

## Model Configuration File

Models are defined in:
```
backend/src/intric/server/dependencies/ai_models.yml
```

### Configuration Structure

```yaml
completion_models:
  - name: 'model-identifier'    # ⚠️ SYNC KEY - Never change!
    nickname: 'Display Name'     # Safe to change (display only)
    family: 'openai'             # Provider family
    token_limit: 128000          # Context window size
    is_deprecated: false         # Mark deprecated instead of removing
    stability: 'stable'          # stable | experimental
    hosting: 'usa'               # usa | eu | swe
    description: 'Model description'
    org: 'Provider Name'
    vision: true                 # Supports image input
    reasoning: false             # Enhanced reasoning capabilities
    litellm_model_name: 'provider/model'  # Optional: LiteLLM routing

embedding_models:
  - name: 'embedding-model'      # ⚠️ SYNC KEY + DISPLAY NAME
    family: 'openai'
    max_input: 8191
    max_batch_size: 32
    dimensions: 512              # Optional
    litellm_model_name: 'provider/model'  # Optional: LiteLLM routing
```

### Critical Warning

> **Never change the `name` field** of an existing model. This field is the database sync key. Changing it will orphan all existing references (assistants, collections, embeddings, etc.) and cause data loss.

**Safe changes:**
- `nickname` (completion models only)
- `description`, `token_limit`, `stability`
- `is_deprecated: true` (instead of removing)

**Unsafe changes:**
- Changing `name` field (orphans data)
- Removing models (breaks references)

---

## Currently Available Models

### Completion Models

#### OpenAI

| Model | Display Name | Context | Features |
|-------|-------------|---------|----------|
| `gpt-4-turbo` | GPT-4 | 128K | Vision |
| `gpt-4o` | GPT-4o | 128K | Vision, Multimodal |
| `gpt-4o-mini` | GPT-4o mini | 128K | Vision |
| `gpt-3.5-turbo` | ChatGPT | 16K | Fast, economical |
| `o3-mini` | o3-mini | 200K | Reasoning |

#### Anthropic Claude

| Model | Display Name | Context | Features |
|-------|-------------|---------|----------|
| `claude-3-opus-latest` | Claude 3 Opus | 200K | Vision, Most capable |
| `claude-3-5-sonnet-latest` | Claude 3.5 Sonnet | 200K | Vision |
| `claude-3-7-sonnet-latest` | Claude 3.7 Sonnet | 200K | Vision, Latest |
| `claude-3-sonnet-20240229` | Claude 3 Sonnet | 200K | Vision, Balanced |
| `claude-3-haiku-20240307` | Claude 3 Haiku | 200K | Vision, Fast |

#### Azure OpenAI (Swedish Hosting)

| Model | Display Name | Context | Features |
|-------|-------------|---------|----------|
| `gpt-4-azure` | GPT-4 (Azure) | 128K | Vision |
| `gpt-4o-azure` | GPT-4o (Azure) | 128K | Vision |
| `gpt-4o-mini-azure` | GPT-4o mini (Azure) | 128K | Vision |
| `o3-mini-azure` | o3-mini (Azure) | 200K | Reasoning |
| `gpt-5-azure` | GPT-5 (Azure) | 400K | Vision, Reasoning |
| `gpt-5-mini-azure` | GPT-5 mini (Azure) | 400K | Vision, Reasoning |
| `gpt-5-nano-azure` | GPT-5 nano (Azure) | 400K | Vision, Reasoning |

#### Swedish Providers

| Model | Display Name | Context | Provider |
|-------|-------------|---------|----------|
| `gemma3-27b-it` | Gemma 3 27B | 128K | GDM.se |

#### European Providers

| Model | Display Name | Context | Provider |
|-------|-------------|---------|----------|
| `Meta-Llama-3_3-70B-Instruct` | Llama 3.3 | 128K | OVHCloud |
| `mistral-large-latest` | Mistral Large | 131K | Mistral |
| `google/gemma-3-27b-it` | Gemma 3 | 128K | vLLM self-hosted |

### Embedding Models

| Model | Family | Max Input | Hosting | Provider |
|-------|--------|-----------|---------|----------|
| `text-embedding-3-small` | openai | 8191 | USA | OpenAI |
| `text-embedding-ada-002` | openai | 8191 | USA | OpenAI |
| `text-embedding-3-large-azure` | openai | 8191 | Sweden | Azure |
| `multilingual-e5-large` | e5 | 1400 | Sweden | Berget.ai |
| `multilingual-e5-large-instruct` | e5 | 1400 | Sweden | GDM.se |

---

## Provider Setup

### OpenAI

Add to your `.env` file:

```bash
OPENAI_API_KEY=sk-...your-api-key
```

Get your key from [OpenAI Platform](https://platform.openai.com/).

### Anthropic (Claude)

```bash
ANTHROPIC_API_KEY=sk-ant-...your-api-key
```

Get your key from [Anthropic Console](https://console.anthropic.com/).

### Azure OpenAI

```bash
AZURE_OPENAI_API_KEY=your-azure-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
```

Configure in [Azure Portal](https://portal.azure.com).

### Google Gemini

```bash
GOOGLE_API_KEY=AIza...your-api-key
```

Get your key from [Google AI Studio](https://makersuite.google.com/app/apikey).

### Swedish Providers

#### Berget.ai

```bash
BERGET_API_KEY=your-berget-api-key
BERGET_API_BASE=https://api.berget.ai/v1
```

Contact [Berget.ai](https://berget.ai/) for access.

#### GDM.se

```bash
GDM_API_KEY=your-gdm-api-key
# Optional: GDM_API_BASE=https://ai.gdm.se/api/v1
```

Contact [GDM.se](https://ai.gdm.se/) for access.

### Self-Hosted (vLLM)

```bash
VLLM_MODEL_URL=http://localhost:8000
VLLM_API_KEY=your-vllm-api-key
```

Deploy vLLM:
```bash
docker run --gpus all -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model google/gemma-3-27b-it
```

### Local Models (Ollama)

```bash
OLLAMA_BASE_URL=http://localhost:11434
```

Install from [ollama.ai](https://ollama.ai).

---

## Adding New Models

### Step 1: Edit ai_models.yml

Add your model to the appropriate section:

**For a standard provider model:**
```yaml
completion_models:
  - name: 'new-model-identifier'  # Unique, never change
    nickname: 'New Model Display Name'
    family: 'openai'              # openai, azure, claude, mistral, vllm
    token_limit: 128000
    is_deprecated: false
    stability: 'experimental'
    hosting: 'usa'
    description: 'Description of the model'
    org: 'Provider'
    vision: false
    reasoning: false
```

**For a LiteLLM-routed model (recommended for custom providers):**
```yaml
completion_models:
  - name: 'custom-model'
    nickname: 'Custom Model'
    family: 'openai'              # Uses OpenAI-compatible API
    token_limit: 128000
    hosting: 'swe'
    litellm_model_name: 'gdm/model-name'  # Routes through LiteLLM
    description: 'Custom model via GDM'
    org: GDM
```

**For an embedding model:**
```yaml
embedding_models:
  - name: 'new-embedding-model'   # Also the display name!
    family: 'openai'              # openai or e5
    max_input: 8191
    max_batch_size: 32
    dimensions: 1536              # Optional
    hosting: 'usa'
    litellm_model_name: 'openai/text-embedding-3-large'
    org: OpenAI
```

### Step 2: Sync with Database

Run the seeding command to sync your changes:

```bash
uv run python -m intric.cli.seed_ai_models
```

This command:
- Compares YAML with database
- Creates new models
- Updates existing model metadata
- Marks removed models (warns but doesn't delete by default)

### Step 3: Restart Backend

```bash
docker compose restart backend
```

---

## LiteLLM Provider Prefixes

When using `litellm_model_name`, use these prefixes:

| Prefix | Provider | Example |
|--------|----------|---------|
| `azure/` | Azure OpenAI | `azure/gpt-4` |
| `anthropic/` | Anthropic | `anthropic/claude-3-opus` |
| `bedrock/` | AWS Bedrock | `bedrock/anthropic.claude-v2` |
| `vertex_ai/` | Google Vertex | `vertex_ai/gemini-pro` |
| `berget/` | Berget.ai | `berget/intfloat/multilingual-e5-large` |
| `gdm/` | GDM.se | `gdm/gemma3-27b-it` |
| (none) | OpenAI default | `gpt-4-turbo` |

See [LiteLLM Providers](https://docs.litellm.ai/docs/providers) for full list.

---

## Model Selection Guidelines

### For General Use
- **GPT-4o** or **Claude 3.5 Sonnet** - Best balance of capability and speed

### For Swedish Data Residency
- **GPT-4o (Azure)** - Swedish Azure region
- **Gemma 3 27B (GDM)** - Swedish-hosted open model
- **Multilingual E5 (Berget/GDM)** - Swedish-hosted embeddings

### For Cost Optimization
- **GPT-3.5 Turbo** - Fast and economical
- **Claude 3 Haiku** - Fast responses, lower cost

### For Complex Reasoning
- **o3-mini** - OpenAI's reasoning model
- **Claude 3 Opus** - Best for complex tasks

### For Self-Hosting
- **vLLM** with Gemma/Llama models
- **Ollama** for local development

---

## Multi-Tenant Configuration

In multi-tenant deployments, each tenant can have their own API credentials:

```bash
# Enable per-tenant credentials
TENANT_CREDENTIALS_ENABLED=true
ENCRYPTION_KEY=your-fernet-encryption-key
```

Configure tenant credentials via the admin API:

```bash
curl -X PUT "https://api.your-domain.com/api/v1/sysadmin/tenants/{tenant_id}/credentials" \
  -H "X-Super-Api-Key: $SUPER_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "openai_api_key": "sk-tenant-specific-key",
    "anthropic_api_key": "sk-ant-tenant-key"
  }'
```

---

## Deprecated Providers

> **Note:** These providers have configurations available but are not actively maintained.

### Mistral AI (Deprecated)

```bash
MISTRAL_API_KEY=your-mistral-api-key
```

### OVHCloud (Deprecated)

```bash
OVHCLOUD_API_KEY=your-ovhcloud-api-key
```

### Flux (Image Generation) - Deprecated

```bash
FLUX_API_KEY=your-flux-api-key
```

### Tavily (Web Search) - Deprecated

```bash
TAVILY_API_KEY=your-tavily-api-key
```

---

## Troubleshooting

### Model Not Available

1. Check that the model exists in `ai_models.yml`
2. Run `uv run python -m intric.cli.seed_ai_models` to sync
3. Verify API key is configured for the provider
4. Check model `is_deprecated` is not `true`

### API Key Errors

- Verify key is correctly set in environment
- Check for extra spaces or line breaks
- Ensure key has correct permissions
- Verify provider account is active

### Rate Limits

- Implement request queuing
- Upgrade provider tier
- Use multiple providers for load balancing

### Connection Errors (Local Models)

- Verify Ollama/vLLM is running
- Check URL and port configuration
- Verify network/firewall settings

---

## Need Help?

- Check the [security guide](/guides/security-compliance)
- Visit [GitHub Issues](https://github.com/eneo-ai/eneo/issues)
- Provider documentation:
  - [OpenAI Docs](https://platform.openai.com/docs)
  - [Anthropic Docs](https://docs.anthropic.com)
  - [Azure OpenAI Docs](https://learn.microsoft.com/azure/ai-services/openai/)
  - [LiteLLM Docs](https://docs.litellm.ai)
  - [Berget.ai](https://berget.ai/)
  - [GDM.se](https://ai.gdm.se/)
  - [vLLM Docs](https://docs.vllm.ai)
  - [Ollama Docs](https://ollama.ai/docs)
